{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYJ0jaaYGfqOJKGw9cLwnT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Konpoulos/Prediciton/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIkEBkjBIdZv"
      },
      "source": [
        "### Exercise 2.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2NbF8dzIoq-"
      },
      "source": [
        "Import and inspect the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p7XSQ_-IuHG"
      },
      "source": [
        "data = pd.read_pickle(\"labeled_tweets.p\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZjhHwuvIvii",
        "outputId": "67a87b15-5ed6-45d6-b98f-0f70ab03617c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_a</th>\n",
              "      <th>text_b</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RT @WaysMeansCmte: Republican Senators need to...</td>\n",
              "      <td>Laid-off workers set up soup kitchens in front...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jeff Van Drew sold out his district and his co...</td>\n",
              "      <td>Pitch in to help Amy Kennedy defeat Jeff Van D...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Speaker Pelosi has failed the American people‚Äî...</td>\n",
              "      <td>House Minority Leader McCarthy: Pelosi touts D...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>To learn more about global efforts to #EndPoli...</td>\n",
              "      <td>Home | End Polio. With your help, we can end p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @realDailyWire: BREAKING: Hunter Biden Rece...</td>\n",
              "      <td>Hunter Biden Received Millions From Wife Of Ex...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              text_a  ... label\n",
              "0  RT @WaysMeansCmte: Republican Senators need to...  ...     2\n",
              "1  Jeff Van Drew sold out his district and his co...  ...     0\n",
              "2  Speaker Pelosi has failed the American people‚Äî...  ...     1\n",
              "3  To learn more about global efforts to #EndPoli...  ...     1\n",
              "4  RT @realDailyWire: BREAKING: Hunter Biden Rece...  ...     0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1EdBNk1IyRb",
        "outputId": "58e368cb-3239-43b7-9eef-f888e2e90891",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(data['label'].value_counts())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    1377\n",
            "0    1275\n",
            "2     230\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68lD8fDOI3oa"
      },
      "source": [
        "(0 = affirmative, 1 = negotiated, 2 =oppositional)\n",
        "So in our dataset there are 1377 valued negotiated 1275 valued affirmative and 230 valued oppositional "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTx3cBwEI72v"
      },
      "source": [
        "#### 1    Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGscTXNWI-yV"
      },
      "source": [
        "#### 1.1.Train a logistic regression classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlQ7sT4aIyVw"
      },
      "source": [
        "#first we will pick our train and test set in 80:20 proportion\n",
        "X = list(data.text_a.values)\n",
        "y = list(data.label.values)# the labels we want to predict --> Y\n",
        "labels = ['affirmative', 'negotiated','oppositional']\n",
        "\n",
        "X_train_str, X_test_str, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5754cM-5Iycd",
        "outputId": "ce5d9ad5-b291-481d-8503-0957e06aacc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#then we make a vocabulary of the corpus\n",
        "cv = CountVectorizer() # this initializes the CountVectorizer \n",
        "\n",
        "cv.fit(X_train_str) # create the vocabulary (using only train set!! beacause we dont want to use the test set my opinion)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmz1wknUIyhH"
      },
      "source": [
        "#we will now create our arrays\n",
        "X_train = cv.transform(X_train_str)\n",
        "X_test = cv.transform(X_test_str)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSfMQ84NIyfQ"
      },
      "source": [
        "#print(X_train.toarray()[0]) #if we want to visualize the array "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zELialacIyaZ",
        "outputId": "d14c52e7-ac3a-4ba9-84f5-3f2408f2a201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "#if we want the results of the CountVectorizer\n",
        "vocabulary = cv.get_feature_names()\n",
        "vectorized_texts = pd.DataFrame(X_train.toarray(), columns=vocabulary)\n",
        "vectorized_texts.head(5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>01dju1d7qc</th>\n",
              "      <th>02</th>\n",
              "      <th>030dnk8aky</th>\n",
              "      <th>040tozx3x9</th>\n",
              "      <th>04knsnijuq</th>\n",
              "      <th>04mohpm9q0</th>\n",
              "      <th>05</th>\n",
              "      <th>05dagzqjuq</th>\n",
              "      <th>06nrbvul11</th>\n",
              "      <th>07</th>\n",
              "      <th>083a0nctqi</th>\n",
              "      <th>0a1szja4du</th>\n",
              "      <th>0b8yk5tqgp</th>\n",
              "      <th>0b9xg5cjgo</th>\n",
              "      <th>0cbhyeincj</th>\n",
              "      <th>0d0a9e8zlv</th>\n",
              "      <th>0dfxvmrmq1</th>\n",
              "      <th>0eqq6os5n0</th>\n",
              "      <th>0ffyamd1wq</th>\n",
              "      <th>0hccolqkju</th>\n",
              "      <th>0hg9ucakw4</th>\n",
              "      <th>0hmez8qfnw</th>\n",
              "      <th>0hn4cdddrd</th>\n",
              "      <th>0jfrubg1k1</th>\n",
              "      <th>0jw395x4uv</th>\n",
              "      <th>0kdmfpt8re</th>\n",
              "      <th>0l5budzhgd</th>\n",
              "      <th>0mnnmeixli</th>\n",
              "      <th>0mzpbfdts0</th>\n",
              "      <th>0n2g75fqao</th>\n",
              "      <th>0oaxokcsrv</th>\n",
              "      <th>0oodrmlfiq</th>\n",
              "      <th>0qrnqdyjz3</th>\n",
              "      <th>0r5c4hzxxj</th>\n",
              "      <th>0ro3rtmp68</th>\n",
              "      <th>0s4sdj0bx2</th>\n",
              "      <th>0s53ezmsf3</th>\n",
              "      <th>0tqjbnrrct</th>\n",
              "      <th>0tvkhalir2</th>\n",
              "      <th>...</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zopj1vqyf5</th>\n",
              "      <th>zpjhcp8xhm</th>\n",
              "      <th>zpvnouzvua</th>\n",
              "      <th>zqax9a0plr</th>\n",
              "      <th>zqm2k7mcda</th>\n",
              "      <th>zqozpuga57</th>\n",
              "      <th>zqvpqkvvr1</th>\n",
              "      <th>zrmt456ihi</th>\n",
              "      <th>zrplmnmqal</th>\n",
              "      <th>zsdmh6rr8w</th>\n",
              "      <th>zsjyvozbsy</th>\n",
              "      <th>zspfivyhna</th>\n",
              "      <th>zsphzbm2fh</th>\n",
              "      <th>zsuic5se9c</th>\n",
              "      <th>ztkmqz19le</th>\n",
              "      <th>ztpyttqfxu</th>\n",
              "      <th>zu</th>\n",
              "      <th>zumkdntoxc</th>\n",
              "      <th>zushj0txja</th>\n",
              "      <th>zuzanacaputova</th>\n",
              "      <th>zv7t8ewhqn</th>\n",
              "      <th>zviizsdhvm</th>\n",
              "      <th>zvtbu7dv69</th>\n",
              "      <th>zw1ks5vzy7</th>\n",
              "      <th>zwnulyoyqi</th>\n",
              "      <th>zwyra1robg</th>\n",
              "      <th>zxdwlo1fha</th>\n",
              "      <th>zxfmrxrwvo</th>\n",
              "      <th>zxnoztibx7</th>\n",
              "      <th>zxqv1atnft</th>\n",
              "      <th>zxxsznkil9</th>\n",
              "      <th>zyafklfryj</th>\n",
              "      <th>zyahjnplbe</th>\n",
              "      <th>zzjwvjrdtd</th>\n",
              "      <th>zzkhhjk8yh</th>\n",
              "      <th>zzm2owgnv3</th>\n",
              "      <th>√°√±ez</th>\n",
              "      <th>√ºber</th>\n",
              "      <th>ùì∏ùìæùìª</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 13406 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   000  01dju1d7qc  02  030dnk8aky  ...  zzm2owgnv3  √°√±ez  √ºber  ùì∏ùìæùìª\n",
              "0    0           0   0           0  ...           0     0     0    0\n",
              "1    0           0   0           0  ...           0     0     0    0\n",
              "2    0           0   0           0  ...           0     0     0    0\n",
              "3    0           0   0           0  ...           0     0     0    0\n",
              "4    0           0   0           0  ...           0     0     0    0\n",
              "\n",
              "[5 rows x 13406 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SCYYqkcJG-P",
        "outputId": "e95f304d-8fd0-4aed-cb7e-fd3aff462cb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#now we will train our model \n",
        "lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfFbsg8-JHAg",
        "outputId": "fc50facd-a9ab-4f62-e407-ac79f93e07fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#now we evaluate the performance \n",
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred, \n",
        "                          target_names=labels))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " affirmative       0.57      0.62      0.60       256\n",
            "  negotiated       0.55      0.59      0.57       270\n",
            "oppositional       0.20      0.04      0.07        51\n",
            "\n",
            "    accuracy                           0.55       577\n",
            "   macro avg       0.44      0.42      0.41       577\n",
            "weighted avg       0.53      0.55      0.54       577\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSL-pzPrJO4-"
      },
      "source": [
        "Evaluation of the model: we see that we have almost 0.6 f1-score which is the mean of precision and recall for affirmative and negotiated but only 0.07 f1-score for oppositional which is far from good score\n",
        "Lets try the random relecting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLDcH5QCIyX7",
        "outputId": "7dcbc982-df31-4b77-c9d9-2c2838d1d9e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#random selecting\n",
        "random_preds = [random.randint(0,2) for i in range(len(y_test))]\n",
        "\n",
        "print(classification_report(y_test, random_preds, \n",
        "                          target_names=labels))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " affirmative       0.50      0.37      0.42       256\n",
            "  negotiated       0.48      0.34      0.40       270\n",
            "oppositional       0.08      0.29      0.12        51\n",
            "\n",
            "    accuracy                           0.35       577\n",
            "   macro avg       0.35      0.33      0.31       577\n",
            "weighted avg       0.45      0.35      0.38       577\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vttWZdOcJVEV"
      },
      "source": [
        "We see that we have better f1-score for oppositional but worse score for the other two."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70UF_69mJXXj"
      },
      "source": [
        "#### 1.2.Try to interpret the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8dItTmQJPt1",
        "outputId": "cd79f1aa-8e18-4cf2-f6b9-0bac00fa6aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "vocabulary = cv.get_feature_names()\n",
        "regression_coefficients = lr.coef_[0] # get the LR weights we have 3 types so 0 is the affirmative\n",
        "vocab_coef_combined = list(zip(regression_coefficients, vocabulary)) # this combines two separate lists [1, 2], ['word1', 'word2'] into one list [[1, 'word1'], [2, 'word2']]\n",
        "\n",
        "feature_importance = pd.DataFrame(vocab_coef_combined,\n",
        "                      columns=['coef', 'word'])\n",
        "feature_importance.sort_values('coef', ascending=False).head(10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5583</th>\n",
              "      <td>0.738226</td>\n",
              "      <td>helping</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10305</th>\n",
              "      <td>0.713530</td>\n",
              "      <td>rt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2839</th>\n",
              "      <td>0.704435</td>\n",
              "      <td>complete</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3189</th>\n",
              "      <td>0.635509</td>\n",
              "      <td>cruz</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1820</th>\n",
              "      <td>0.607602</td>\n",
              "      <td>below</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6493</th>\n",
              "      <td>0.595849</td>\n",
              "      <td>job</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5114</th>\n",
              "      <td>0.590124</td>\n",
              "      <td>generation</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11553</th>\n",
              "      <td>0.585061</td>\n",
              "      <td>team</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7098</th>\n",
              "      <td>0.581903</td>\n",
              "      <td>live</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2887</th>\n",
              "      <td>0.578070</td>\n",
              "      <td>confirmation</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           coef          word\n",
              "5583   0.738226       helping\n",
              "10305  0.713530            rt\n",
              "2839   0.704435      complete\n",
              "3189   0.635509          cruz\n",
              "1820   0.607602         below\n",
              "6493   0.595849           job\n",
              "5114   0.590124    generation\n",
              "11553  0.585061          team\n",
              "7098   0.581903          live\n",
              "2887   0.578070  confirmation"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvq-3d6PJcWO"
      },
      "source": [
        "We can see that for the word helping has the highest weight for affirmative titles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHKRNfCaJPv4",
        "outputId": "bac4adcc-3ba9-4d5e-e0ee-d4ee27851ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "vocabulary = cv.get_feature_names()\n",
        "regression_coefficients = lr.coef_[1] # get the LR weights we have 3 types so 0 is the affirmative\n",
        "vocab_coef_combined = list(zip(regression_coefficients, vocabulary)) # this combines two separate lists [1, 2], ['word1', 'word2'] into one list [[1, 'word1'], [2, 'word2']]\n",
        "\n",
        "feature_importance = pd.DataFrame(vocab_coef_combined,\n",
        "                      columns=['coef', 'word'])\n",
        "feature_importance.sort_values('coef', ascending=False).head(10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5371</th>\n",
              "      <td>0.719120</td>\n",
              "      <td>guard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8720</th>\n",
              "      <td>0.665121</td>\n",
              "      <td>passing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9791</th>\n",
              "      <td>0.613768</td>\n",
              "      <td>remains</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12778</th>\n",
              "      <td>0.596087</td>\n",
              "      <td>were</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11886</th>\n",
              "      <td>0.588955</td>\n",
              "      <td>trade</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9072</th>\n",
              "      <td>0.582236</td>\n",
              "      <td>post</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2049</th>\n",
              "      <td>0.578450</td>\n",
              "      <td>box</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8701</th>\n",
              "      <td>0.577499</td>\n",
              "      <td>partisan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7474</th>\n",
              "      <td>0.554761</td>\n",
              "      <td>means</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8969</th>\n",
              "      <td>0.552788</td>\n",
              "      <td>please</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           coef      word\n",
              "5371   0.719120     guard\n",
              "8720   0.665121   passing\n",
              "9791   0.613768   remains\n",
              "12778  0.596087      were\n",
              "11886  0.588955     trade\n",
              "9072   0.582236      post\n",
              "2049   0.578450       box\n",
              "8701   0.577499  partisan\n",
              "7474   0.554761     means\n",
              "8969   0.552788    please"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsJc8nnIJfOp"
      },
      "source": [
        "We can see that for the word guard has the highest weight for negotiated titles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnXtZbUaJPzZ",
        "outputId": "6d180aa4-39c2-4e83-e91d-9f807b4f5d2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "vocabulary = cv.get_feature_names()\n",
        "regression_coefficients = lr.coef_[2] # get the LR weights we have 3 types so 0 is the affirmative\n",
        "vocab_coef_combined = list(zip(regression_coefficients, vocabulary)) # this combines two separate lists [1, 2], ['word1', 'word2'] into one list [[1, 'word1'], [2, 'word2']]\n",
        "\n",
        "feature_importance = pd.DataFrame(vocab_coef_combined,\n",
        "                      columns=['coef', 'word'])\n",
        "feature_importance.sort_values('coef', ascending=False).head(10)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1445</th>\n",
              "      <td>0.606015</td>\n",
              "      <td>around</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5598</th>\n",
              "      <td>0.604660</td>\n",
              "      <td>heroes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2475</th>\n",
              "      <td>0.594943</td>\n",
              "      <td>chairman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12149</th>\n",
              "      <td>0.585841</td>\n",
              "      <td>unacceptable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5020</th>\n",
              "      <td>0.573001</td>\n",
              "      <td>fvmwrb09nu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9862</th>\n",
              "      <td>0.573001</td>\n",
              "      <td>repdonbeyer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7906</th>\n",
              "      <td>0.573001</td>\n",
              "      <td>mypuntxhcr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1673</th>\n",
              "      <td>0.554696</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8198</th>\n",
              "      <td>0.544240</td>\n",
              "      <td>not</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6325</th>\n",
              "      <td>0.541231</td>\n",
              "      <td>issues</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           coef          word\n",
              "1445   0.606015        around\n",
              "5598   0.604660        heroes\n",
              "2475   0.594943      chairman\n",
              "12149  0.585841  unacceptable\n",
              "5020   0.573001    fvmwrb09nu\n",
              "9862   0.573001   repdonbeyer\n",
              "7906   0.573001    mypuntxhcr\n",
              "1673   0.554696           bad\n",
              "8198   0.544240           not\n",
              "6325   0.541231        issues"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KFudjvCJlLa"
      },
      "source": [
        "We can see that for the word around has the highest weight for oppositional titles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXW0xrFoJnss"
      },
      "source": [
        "#### 1.3.Use TF-IDF features instead of raw counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kTDSEmvIyTe"
      },
      "source": [
        "tfidf = TfidfVectorizer() \n",
        "\n",
        "tfidf.fit(X_train_str) # create the vocabulary\n",
        "\n",
        "X_train_idf = tfidf.transform(X_train_str)\n",
        "X_test = tfidf.transform(X_test_str)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWjh31Z2Js9_",
        "outputId": "cf56cad6-9e7c-4557-f8ed-729f77166d91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "lr = LogisticRegression(solver='lbfgs',max_iter = 1000)\n",
        "lr.fit(X_train_idf, y_train)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwkIgs97JtCG",
        "outputId": "92997fb2-1974-4a04-f294-ffc40dfc9dba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred, \n",
        "                          target_names=labels))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " affirmative       0.64      0.56      0.60       256\n",
            "  negotiated       0.55      0.72      0.62       270\n",
            "oppositional       0.00      0.00      0.00        51\n",
            "\n",
            "    accuracy                           0.59       577\n",
            "   macro avg       0.40      0.43      0.41       577\n",
            "weighted avg       0.54      0.59      0.56       577\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wtuf1GcBJtAc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJKne3vEIh6w"
      },
      "source": [
        "#### Exercise 2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT_VxzF1J2XU"
      },
      "source": [
        "### 2 BERT: supervised \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsF3ydD96K8b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import random\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizerFast\n",
        "import tensorflow as tf\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "from  transformers  import  AutoTokenizer\n",
        "from  transformers  import TFAutoModelForSequenceClassification\n",
        "from transformers import pipeline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJwqYjdK7r6a"
      },
      "source": [
        "data = pd.read_pickle(\"labeled_tweets.p\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH1Ccb6NUfN2",
        "outputId": "f15b744a-65db-494c-d03c-1dd2df6474cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text_a</th>\n",
              "      <th>text_b</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>RT @WaysMeansCmte: Republican Senators need to...</td>\n",
              "      <td>Laid-off workers set up soup kitchens in front...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jeff Van Drew sold out his district and his co...</td>\n",
              "      <td>Pitch in to help Amy Kennedy defeat Jeff Van D...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Speaker Pelosi has failed the American people‚Äî...</td>\n",
              "      <td>House Minority Leader McCarthy: Pelosi touts D...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>To learn more about global efforts to #EndPoli...</td>\n",
              "      <td>Home | End Polio. With your help, we can end p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @realDailyWire: BREAKING: Hunter Biden Rece...</td>\n",
              "      <td>Hunter Biden Received Millions From Wife Of Ex...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              text_a  ... label\n",
              "0  RT @WaysMeansCmte: Republican Senators need to...  ...     2\n",
              "1  Jeff Van Drew sold out his district and his co...  ...     0\n",
              "2  Speaker Pelosi has failed the American people‚Äî...  ...     1\n",
              "3  To learn more about global efforts to #EndPoli...  ...     1\n",
              "4  RT @realDailyWire: BREAKING: Hunter Biden Rece...  ...     0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTXpXPLu7BDu"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88H5ej9m85b8"
      },
      "source": [
        "X = list(data.text_a.values) # the texts --> X\n",
        "y = list(data.label.values) # the labels we want to predict --> Y\n",
        "labels = ['affirmative', 'negotiated','oppositional']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H2_0Y92_MA2"
      },
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128) # convert input strings to BERT encodings\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True,  max_length=128)\n",
        "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        ")).shuffle(1000).batch(16) # convert the encodings to Tensorflow objects\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    y_val\n",
        ")).batch(64)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    y_test\n",
        ")).batch(64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDQk_gqC_wjn"
      },
      "source": [
        "Now we Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llcaVaO3_X9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75643635-680f-4371-8069-1b30d98e8919"
      },
      "source": [
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-cased',num_labels=len(labels))\n",
        "callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, \n",
        "                      mode='min', baseline=None, \n",
        "                      restore_best_weights=True)]\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_77']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8BJj130_51m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdd41fff-f94e-4feb-dcdc-f64952ee0535"
      },
      "source": [
        "model.fit(train_dataset, \n",
        "            epochs=10,\n",
        "          callbacks=callbacks, \n",
        "          validation_data=val_dataset,\n",
        "           batch_size=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "145/145 [==============================] - 35s 241ms/step - loss: 0.9216 - val_loss: 0.8805\n",
            "Epoch 2/10\n",
            "145/145 [==============================] - 33s 225ms/step - loss: 0.8859 - val_loss: 0.8820\n",
            "Epoch 3/10\n",
            "145/145 [==============================] - 33s 227ms/step - loss: 0.8436 - val_loss: 0.8685\n",
            "Epoch 4/10\n",
            "145/145 [==============================] - 33s 227ms/step - loss: 0.7409 - val_loss: 0.9026\n",
            "Epoch 5/10\n",
            "145/145 [==============================] - 33s 226ms/step - loss: 0.5478 - val_loss: 0.9934\n",
            "Epoch 6/10\n",
            "145/145 [==============================] - 33s 227ms/step - loss: 0.3729 - val_loss: 1.4913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb7d0426c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spUfzM_KAtAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007ef6a2-545f-416c-8a05-0aa42b3e5877"
      },
      "source": [
        "logits = model.predict(test_dataset)\n",
        "y_preds = np.argmax(logits[0], axis=1)\n",
        "print(classification_report(y_test, y_preds))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.58      0.57       116\n",
            "           1       0.61      0.70      0.65       147\n",
            "           2       0.00      0.00      0.00        25\n",
            "\n",
            "    accuracy                           0.59       288\n",
            "   macro avg       0.39      0.43      0.41       288\n",
            "weighted avg       0.54      0.59      0.56       288\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKtkxm0RMnOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8656419-a4ea-42a4-f51f-339d94e7ba18"
      },
      "source": [
        "print(confusion_matrix(y_test, y_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 67  49   0]\n",
            " [ 44 103   0]\n",
            " [  7  18   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9UH4DuGCTu2"
      },
      "source": [
        "we do have a slitly better f1-score for negotiated to 0.65 but other than that not much of a change\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_iT_wDGDX17"
      },
      "source": [
        "2.2  Use  of  BERTweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nil4u6_3BrGl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b9a8aa1-d107-4039-87fc-d8bf7ec57ce0"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrtgdwSuM-ZJ"
      },
      "source": [
        "X = list(data.text_a.values) # the texts --> X\n",
        "y = list(data.label.values) # the labels we want to predict --> Y\n",
        "labels = ['affirmative', 'negotiated','oppositional']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4FGrp6EE9XE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d452544-0127-4805-8b7a-519937dd60f6"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base' , normalization=True)\n",
        "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128) # convert input strings to BERT encodings\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True,  max_length=128)\n",
        "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        ")).shuffle(1000).batch(16) # convert the encodings to Tensorflow objects\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    y_val\n",
        ")).batch(64)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    y_test\n",
        ")).batch(64)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO2-pCr9I08x"
      },
      "source": [
        "We now compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smT5fJhZHUfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4af2d602-47cb-4bbf-8978-f6ac080c82f9"
      },
      "source": [
        "model = TFAutoModelForSequenceClassification.from_pretrained('vinai/bertweet-base', num_labels=len(labels))\n",
        "callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, \n",
        "                      mode='min', baseline=None, \n",
        "                      restore_best_weights=True)]\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at vinai/bertweet-base were not used when initializing TFRobertaForSequenceClassification: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkEIdaIMJLtw"
      },
      "source": [
        "Train the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnq591wPI_Jd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0785a273-ecc3-4d60-b766-bb7fad44f7b3"
      },
      "source": [
        "model.roberta.return_dict = False\n",
        "model.fit(train_dataset, \n",
        "            epochs=10,\n",
        "          callbacks=callbacks, \n",
        "          validation_data=val_dataset,\n",
        "           batch_size=16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "145/145 [==============================] - 68s 471ms/step - loss: 0.9020 - val_loss: 0.9753\n",
            "Epoch 2/10\n",
            "145/145 [==============================] - 66s 454ms/step - loss: 0.8629 - val_loss: 0.8242\n",
            "Epoch 3/10\n",
            "145/145 [==============================] - 65s 451ms/step - loss: 0.7905 - val_loss: 0.7975\n",
            "Epoch 4/10\n",
            "145/145 [==============================] - 65s 448ms/step - loss: 0.6915 - val_loss: 0.9590\n",
            "Epoch 5/10\n",
            "145/145 [==============================] - 65s 449ms/step - loss: 0.5436 - val_loss: 1.1634\n",
            "Epoch 6/10\n",
            "145/145 [==============================] - 65s 451ms/step - loss: 0.3951 - val_loss: 1.1452\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb6c7faa1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCKe9fooLrAN"
      },
      "source": [
        "We evaluate the model now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDkOxJ45KSwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b553423-d789-42e8-837a-fa32eebf6c0b"
      },
      "source": [
        "logits = model.predict(test_dataset)\n",
        "y_preds = np.argmax(logits[0], axis=1)\n",
        "print(classification_report(y_test, y_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.73      0.63       116\n",
            "           1       0.63      0.58      0.60       147\n",
            "           2       0.00      0.00      0.00        25\n",
            "\n",
            "    accuracy                           0.59       288\n",
            "   macro avg       0.40      0.44      0.41       288\n",
            "weighted avg       0.55      0.59      0.56       288\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaSWz8_AL-6d"
      },
      "source": [
        "We have a better result for 0 but worst for 1  for the BERTweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLPEG2jiLuFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03dadbb-35b3-41c1-d154-cd05d1811a59"
      },
      "source": [
        "print(confusion_matrix(y_test, y_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[85 31  0]\n",
            " [62 85  0]\n",
            " [ 6 19  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIFgokfuVOIu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_emvJ-VR8nW"
      },
      "source": [
        "2.4 For the label oppositional we can understand that our model didnt found any prediction so the count number is 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRoCKQHvSaKT"
      },
      "source": [
        "2.5 Tim told this beacause some tweets especially for the elections as we used have some standard words that they using like elections,precident and other , so "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XXRvfO3TCu1"
      },
      "source": [
        "3 Zero shot classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-vBXZDHTcGW"
      },
      "source": [
        "3.1 Think of the topics that were relevant during the U.S. elections in thelast three months.  Use these topics as candidate labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbXxAZ-RMbXX",
        "outputId": "924621b8-9470-4221-d76e-dca0445796f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data['text_a']"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     RT @WaysMeansCmte: Republican Senators need to...\n",
              "1     Jeff Van Drew sold out his district and his co...\n",
              "2     Speaker Pelosi has failed the American people‚Äî...\n",
              "3     To learn more about global efforts to #EndPoli...\n",
              "4     RT @realDailyWire: BREAKING: Hunter Biden Rece...\n",
              "                            ...                        \n",
              "65    For a President who once claimed he wanted to ...\n",
              "66    RT @PAStateDept: While @Eagles‚Äô mascot, Swoop,...\n",
              "67    The only ‚Äúhealthcare plan‚Äù they have is a plan...\n",
              "68    RT @MSyallpolitics: Mississippi Delegation Sup...\n",
              "69    RT @SenatorHaywood: The Black Doctors Consorti...\n",
              "Name: text_a, Length: 2882, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0uwsKtiV4E5",
        "outputId": "234bff75-125d-4f7d-eaa0-0733c3a0abd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "candidate_labels = [\"politics\",\"public\",\"precident\",'health'] #my choice as candidate_labels and then we just pick the 10 first to evaluate\n",
        "df = pd.read_pickle(\"labeled_tweets.p\")\n",
        "df_subset = df.iloc[:10]\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "sequence = list(df_subset.text_a.values)\n",
        "\n",
        "classifier(sequence,candidate_labels)\n",
        " "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n",
            "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
            "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'labels': ['public', 'precident', 'politics', 'health'],\n",
              "  'scores': [0.44837504625320435,\n",
              "   0.3660442531108856,\n",
              "   0.179136723279953,\n",
              "   0.006443963386118412],\n",
              "  'sequence': 'RT @WaysMeansCmte: Republican Senators need to understand the consequences of their inaction. \\n\\nPeople can‚Äôt pay for groceries because the Senate GOP decided $600 was ‚Äútoo much‚Äù for unemployed Americans.\\nhttps://t.co/eOADMIF2Dk'},\n",
              " {'labels': ['politics', 'public', 'precident', 'health'],\n",
              "  'scores': [0.42231225967407227,\n",
              "   0.3081919848918915,\n",
              "   0.25462833046913147,\n",
              "   0.014867417514324188],\n",
              "  'sequence': \"Jeff Van Drew sold out his district and his country to help Donald Trump. He thought it would boost his re-election prospects, but we'll prove him wrong in November.\\n\\n@AmyKennedy715 will be the leader South Jersey deserves. Join me in supporting her: \\nhttps://t.co/G9zO894m3Y\"},\n",
              " {'labels': ['politics', 'precident', 'public', 'health'],\n",
              "  'scores': [0.44012215733528137,\n",
              "   0.23290136456489563,\n",
              "   0.23169143497943878,\n",
              "   0.0952850952744484],\n",
              "  'sequence': 'Speaker Pelosi has failed the American people‚Äîfrom the weakest, most partisan impeachment in history to politicizing the coronavirus pandemic and stopping relief for workers and families, and now perpetuating conspiracy theories about the Postal Service. https://t.co/SpKAnCl6NZ'},\n",
              " {'labels': ['health', 'public', 'precident', 'politics'],\n",
              "  'scores': [0.5499022006988525,\n",
              "   0.3947213590145111,\n",
              "   0.05275849252939224,\n",
              "   0.0026179386768490076],\n",
              "  'sequence': 'To learn more about global efforts to #EndPolio, visit @Rotary, @ShotAtLife, @CDCgov, @USAID, https://t.co/CWdSP7Sv9F, and https://t.co/otIevKE4kJ'},\n",
              " {'labels': ['precident', 'public', 'politics', 'health'],\n",
              "  'scores': [0.4407719671726227,\n",
              "   0.42143717408180237,\n",
              "   0.12632285058498383,\n",
              "   0.011468015611171722],\n",
              "  'sequence': 'RT @realDailyWire: BREAKING: Hunter Biden Received Millions From Wife Of Ex-Moscow Mayor, Paid Suspects Allegedly Tied To Trafficking, Had Contacts With Chinese Military, Senate Report Alleges https://t.co/hdrZjrbYIo https://t.co/FMajfpRQRS'},\n",
              " {'labels': ['public', 'health', 'precident', 'politics'],\n",
              "  'scores': [0.48903581500053406,\n",
              "   0.3041984736919403,\n",
              "   0.19535182416439056,\n",
              "   0.01141391508281231],\n",
              "  'sequence': 'As we continue to navigate the #COVID19 pandemic, the need for Americans to get their flu shots is as important as ever. More Americans w/ flu vaccinations will help keep hospitals from being overwhelmed from the combination of flu &amp; COVID patients. https://t.co/3oTyO2Zq6c QUOTE @adndotcom: Getting a flu shot this year is particularly important, state health officials said. A large outbreak of influenza could put further stress the state‚Äôs fragile health care system. https://t.co/TxKmliJq9I'},\n",
              " {'labels': ['public', 'precident', 'health', 'politics'],\n",
              "  'scores': [0.5243800282478333,\n",
              "   0.41497477889060974,\n",
              "   0.042688868939876556,\n",
              "   0.017956318333745003],\n",
              "  'sequence': 'The #COVID19 pandemic has drastically increased food insecurity in our country. On this #WorldFoodDay, it is another stark reminder of the critical need to pass the #HeroesAct‚Äî we cannot afford to wait any longer in addressing food security in America.\\nhttps://t.co/k0AVlLNXuB'},\n",
              " {'labels': ['public', 'precident', 'politics', 'health'],\n",
              "  'scores': [0.6849474310874939,\n",
              "   0.23460599780082703,\n",
              "   0.05129748955368996,\n",
              "   0.02914910577237606],\n",
              "  'sequence': 'Some people have been asking what the deal is with this big initiative I‚Äôm working on to help Vietnam Vets get the benefits they earned. If you‚Äôre interested, learn more about the push here: https://t.co/IPbAg72o64'},\n",
              " {'labels': ['public', 'precident', 'politics', 'health'],\n",
              "  'scores': [0.7722490429878235,\n",
              "   0.1812485307455063,\n",
              "   0.03677784651517868,\n",
              "   0.00972458254545927],\n",
              "  'sequence': 'RT @SEIU: Communities of color are at risk of being underrepresented in the 2020 Census.\\n\\nFilling out the #census is a quick and simple process that will make a huge difference in the fight for the services we all need. #UnionsForAll https://t.co/N7k7JSirTN'},\n",
              " {'labels': ['public', 'precident', 'health', 'politics'],\n",
              "  'scores': [0.6050826907157898,\n",
              "   0.3664112091064453,\n",
              "   0.018393954262137413,\n",
              "   0.010112157091498375],\n",
              "  'sequence': \"I was honored to join this ceremony with Legion Post #9 and VFW Post #1617 in Derry to honor Gold Star Mother Nancy Geary.\\n\\nWe will always remember the sacrifice of her son, LCpl. Michael Geary, as well as Nancy's sacrifice, courage, and resiliency.\\n\\nhttps://t.co/Yl0Bm4wkyD\"}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Ztdy_8HhR8"
      },
      "source": [
        "We can evluate from the model that the first 2 words that i used \"public\" and \"precident\" where the words with by far the biggest score!And yes i think this model can be used to label the topics of those tweets!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8foFfgzEPXK"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHpI8NgFXJee"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}